#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
Utilities Module - Responsible for auxiliary functions and trading signal generation

This module implements various auxiliary functions, including trading signal generation,
performance evaluation, backtesting simulation, etc.
"""

import numpy as np
import pandas as pd
import os
import logging
import json
import time
from datetime import datetime, timedelta

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger('stock_prediction')


def generate_trading_signals(df, predictions, threshold=0.5, consecutive_days=1):
    """
    Generate trading signals based on prediction results
    
    Parameters:
        df (pandas.DataFrame): Original stock data
        predictions (numpy.ndarray): Prediction results array
        threshold (float): Signal trigger threshold
        consecutive_days (int): Required consecutive prediction days
    
    Returns:
        pandas.DataFrame: DataFrame with added trading signals
    """
    try:
        if df is None or len(df) == 0:
            logger.error("Empty data provided")
            return None
            
        if predictions is None or len(predictions) == 0:
            logger.error("Empty predictions")
            return None
            
        if len(df) != len(predictions):
            logger.error(f"Data length mismatch: df={len(df)}, predictions={len(predictions)}")
            return None
            
        # Copy the DataFrame to avoid modifying the original data
        result_df = df.copy()
        
        # Add prediction results
        result_df['prediction'] = predictions
        
        # Initialize signals
        result_df['signal'] = 0  # 0: No operation, 1: Buy, -1: Sell
        
        # Consecutive up predictions as buy signals
        for i in range(consecutive_days - 1, len(result_df)):
            # Check if the past consecutive_days days have consecutive up predictions
            if all(result_df['prediction'].iloc[i-consecutive_days+1:i+1] >= threshold):
                result_df.loc[result_df.index[i], 'signal'] = 1
                
            # Check if the past consecutive_days days have consecutive down predictions
            elif all(result_df['prediction'].iloc[i-consecutive_days+1:i+1] < threshold):
                result_df.loc[result_df.index[i], 'signal'] = -1
                
        logger.info(f"Generated trading signals: Buy {sum(result_df['signal'] == 1)} times, Sell {sum(result_df['signal'] == -1)} times")
        
        return result_df
        
    except Exception as e:
        logger.error(f"Error generating trading signals: {str(e)}")
        return None


def simulate_trading(df, initial_capital=100000.0, commission_rate=0.0003, slippage=0.001):
    """
    Simulate trading and calculate returns
    
    Parameters:
        df (pandas.DataFrame): DataFrame containing prices and signals
        initial_capital (float): Initial capital
        commission_rate (float): Commission rate
        slippage (float): Slippage rate
    
    Returns:
        tuple: (Total return, Return rate, Trading records)
    """
    try:
        if df is None or len(df) == 0:
            logger.error("Empty data provided")
            return 0, 0, pd.DataFrame()
            
        if 'signal' not in df.columns:
            logger.error("No signal column in data")
            return 0, 0, pd.DataFrame()
            
        if 'close' not in df.columns:
            logger.error("No close price column in data")
            return 0, 0, pd.DataFrame()
            
        # Copy the DataFrame
        backtest_df = df.copy()
        
        # Initialize account status
        capital = initial_capital
        position = 0
        trades = []
        
        # Simulate trading
        for i in range(len(backtest_df)):
            date = backtest_df.index[i] if isinstance(backtest_df.index[i], datetime) else backtest_df['date'].iloc[i]
            price = backtest_df['close'].iloc[i]
            signal = backtest_df['signal'].iloc[i]
            
            # Buy signal
            if signal == 1 and position == 0:
                # Calculate the number of shares to buy (considering commission and slippage)
                buy_price = price * (1 + slippage)
                max_shares = int(capital / buy_price / (1 + commission_rate) / 100) * 100  # Buy in multiples of 100 shares
                
                if max_shares > 0:
                    cost = max_shares * buy_price
                    commission = cost * commission_rate
                    total_cost = cost + commission
                    
                    if total_cost <= capital:
                        position = max_shares
                        capital -= total_cost
                        
                        trades.append({
                            'date': date,
                            'action': 'BUY',
                            'price': buy_price,
                            'shares': max_shares,
                            'commission': commission,
                            'value': cost,
                            'capital': capital
                        })
                        
                        logger.info(f"Buy: {date}, Price: {buy_price:.2f}, Shares: {max_shares}, Commission: {commission:.2f}, Remaining capital: {capital:.2f}")
                        
            # Sell signal
            elif signal == -1 and position > 0:
                # Calculate the sell revenue (considering commission and slippage)
                sell_price = price * (1 - slippage)
                revenue = position * sell_price
                commission = revenue * commission_rate
                net_revenue = revenue - commission
                
                capital += net_revenue
                
                trades.append({
                    'date': date,
                    'action': 'SELL',
                    'price': sell_price,
                    'shares': position,
                    'commission': commission,
                    'value': revenue,
                    'capital': capital
                })
                
                logger.info(f"Sell: {date}, Price: {sell_price:.2f}, Shares: {position}, Commission: {commission:.2f}, Remaining capital: {capital:.2f}")
                
                position = 0
                
        # Force close the position on the last day
        if position > 0:
            price = backtest_df['close'].iloc[-1] * (1 - slippage)
            revenue = position * price
            commission = revenue * commission_rate
            net_revenue = revenue - commission
            
            capital += net_revenue
            
            date = backtest_df.index[-1] if isinstance(backtest_df.index[-1], datetime) else backtest_df['date'].iloc[-1]
            
            trades.append({
                'date': date,
                'action': 'FINAL_SELL',
                'price': price,
                'shares': position,
                'commission': commission,
                'value': revenue,
                'capital': capital
            })
            
            logger.info(f"Final close: {date}, Price: {price:.2f}, Shares: {position}, Commission: {commission:.2f}, Remaining capital: {capital:.2f}")
            
        # Calculate total return and return rate
        profit = capital - initial_capital
        profit_rate = profit / initial_capital * 100
        
        # Create trading records DataFrame
        trades_df = pd.DataFrame(trades)
        
        logger.info(f"Trading simulation completed: Total return {profit:.2f}, Return rate {profit_rate:.2f}%, Number of trades {len(trades)}")
        
        return profit, profit_rate, trades_df
        
    except Exception as e:
        logger.error(f"Error simulating trading: {str(e)}")
        return 0, 0, pd.DataFrame()


def calculate_metrics(predictions, y_true):
    """
    è®¡ç®—é¢„æµ‹æ€§èƒ½è¯„ä¼°æŒ‡æ ‡ï¼Œç”¨äºè¯„ä¼°æ¨¡å‹é¢„æµ‹æ•ˆæœ
    
    è¯¥å‡½æ•°è®¡ç®—å¤šç§åˆ†ç±»è¯„ä¼°æŒ‡æ ‡ï¼ŒåŒ…æ‹¬å‡†ç¡®ç‡ã€ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1åˆ†æ•°ç­‰ï¼Œ
    é€‚ç”¨äºäºŒå…ƒåˆ†ç±»é—®é¢˜ï¼ˆé¢„æµ‹è‚¡ä»·æ¶¨è·Œï¼‰ã€‚æ‰€æœ‰æŒ‡æ ‡éƒ½åŸºäºæ··æ·†çŸ©é˜µè®¡ç®—å¾—å‡ºã€‚
    
    å‚æ•°:
        predictions (numpy.ndarray): æ¨¡å‹é¢„æµ‹ç»“æœæ•°ç»„ï¼Œå½¢çŠ¶ä¸º(n_samples,)ã€‚
                                  å…¶ä¸­0ä»£è¡¨é¢„æµ‹ä¸‹è·Œï¼Œ1ä»£è¡¨é¢„æµ‹ä¸Šæ¶¨ã€‚æ•°ç»„åº”
                                  åªåŒ…å«0å’Œ1å€¼ï¼Œè¡¨ç¤ºäºŒå…ƒåˆ†ç±»ç»“æœè€Œéæ¦‚ç‡å€¼ã€‚
        
        y_true (numpy.ndarray): çœŸå®æ ‡ç­¾æ•°ç»„ï¼Œå½¢çŠ¶ä¸º(n_samples,)ã€‚
                              å…¶ä¸­0ä»£è¡¨å®é™…ä¸‹è·Œï¼Œ1ä»£è¡¨å®é™…ä¸Šæ¶¨ã€‚è¯¥æ•°ç»„
                              åº”ä¸predictionsæ•°ç»„é•¿åº¦ä¸€è‡´ã€‚
    
    è¿”å›:
        dict: åŒ…å«ä»¥ä¸‹è¯„ä¼°æŒ‡æ ‡çš„å­—å…¸:
            - accuracy (float): å‡†ç¡®ç‡ï¼Œæ­£ç¡®é¢„æµ‹çš„æ ·æœ¬æ¯”ä¾‹ï¼ŒèŒƒå›´0-100(%)
            - precision (float): ç²¾ç¡®ç‡ï¼Œé¢„æµ‹ä¸ºä¸Šæ¶¨ä¸”å®é™…ä¸Šæ¶¨çš„æ¯”ä¾‹ï¼ŒèŒƒå›´0-1
            - recall (float): å¬å›ç‡ï¼Œå®é™…ä¸Šæ¶¨è¢«æ­£ç¡®é¢„æµ‹çš„æ¯”ä¾‹ï¼ŒèŒƒå›´0-1
            - f1 (float): F1åˆ†æ•°ï¼Œç²¾ç¡®ç‡å’Œå¬å›ç‡çš„è°ƒå’Œå¹³å‡å€¼ï¼ŒèŒƒå›´0-1
            - correct_up (int): æ­£ç¡®é¢„æµ‹ä¸Šæ¶¨çš„æ ·æœ¬æ•°é‡
            - correct_down (int): æ­£ç¡®é¢„æµ‹ä¸‹è·Œçš„æ ·æœ¬æ•°é‡
            - confusion_matrix (list): æ··æ·†çŸ©é˜µï¼Œ2Ã—2åˆ—è¡¨æ ¼å¼
                                      [[çœŸå®ä¸‹è·Œé¢„æµ‹ä¸‹è·Œ, çœŸå®ä¸‹è·Œé¢„æµ‹ä¸Šæ¶¨],
                                       [çœŸå®ä¸Šæ¶¨é¢„æµ‹ä¸‹è·Œ, çœŸå®ä¸Šæ¶¨é¢„æµ‹ä¸Šæ¶¨]]
    
    è¯„ä¼°æŒ‡æ ‡è§£é‡Š:
        - å‡†ç¡®ç‡(Accuracy): æ‰€æœ‰é¢„æµ‹ä¸­æ­£ç¡®çš„æ¯”ä¾‹ï¼Œä½†åœ¨ç±»åˆ«ä¸å¹³è¡¡æ—¶å¯èƒ½æœ‰è¯¯å¯¼æ€§
        - ç²¾ç¡®ç‡(Precision): é¢„æµ‹ä¸ºä¸Šæ¶¨çš„æ ·æœ¬ä¸­ï¼Œå®é™…ä¸Šæ¶¨çš„æ¯”ä¾‹ï¼Œåæ˜ æ¨¡å‹é¢„æµ‹ä¸Šæ¶¨ä¿¡å·çš„å¯é æ€§
        - å¬å›ç‡(Recall): å®é™…ä¸Šæ¶¨çš„æ ·æœ¬ä¸­ï¼Œè¢«æˆåŠŸé¢„æµ‹çš„æ¯”ä¾‹ï¼Œåæ˜ æ¨¡å‹æ•è·ä¸Šæ¶¨æœºä¼šçš„èƒ½åŠ›
        - F1åˆ†æ•°: ç²¾ç¡®ç‡å’Œå¬å›ç‡çš„è°ƒå’Œå¹³å‡ï¼Œå¹³è¡¡è€ƒè™‘ä¸¤ä¸ªæŒ‡æ ‡ï¼Œé€‚åˆè¯„ä¼°æ•´ä½“æ€§èƒ½
    
    ç¤ºä¾‹:
        >>> # å‡è®¾æˆ‘ä»¬æœ‰é¢„æµ‹ç»“æœå’ŒçœŸå®æ ‡ç­¾
        >>> predictions = np.array([1, 0, 1, 1, 0, 0, 1, 0])
        >>> y_true = np.array([1, 0, 0, 1, 0, 1, 1, 0])
        >>> metrics = calculate_metrics(predictions, y_true)
        >>> print(f"å‡†ç¡®ç‡: {metrics['accuracy']:.2f}%")
        >>> print(f"ç²¾ç¡®ç‡: {metrics['precision']:.4f}")
        >>> print(f"å¬å›ç‡: {metrics['recall']:.4f}")
        >>> print(f"F1åˆ†æ•°: {metrics['f1']:.4f}")
        >>> print(f"æ··æ·†çŸ©é˜µ:\n{metrics['confusion_matrix']}")
    
    æ³¨æ„:
        1. è¯¥å‡½æ•°ä¸“ä¸ºäºŒå…ƒåˆ†ç±»è®¾è®¡ï¼Œé¢„æµ‹å€¼å’ŒçœŸå®å€¼å¿…é¡»ä¸º0æˆ–1ï¼Œä¸æ”¯æŒå¤šç±»åˆ«æˆ–å›å½’é—®é¢˜
        2. å½“æŸä¸€ç±»åˆ«æ ·æœ¬æ•°ä¸ºé›¶æ—¶ï¼Œç›¸å…³çš„æŒ‡æ ‡(å¦‚ç²¾ç¡®ç‡æˆ–å¬å›ç‡)å¯èƒ½å˜ä¸ºé›¶ï¼Œéœ€è°¨æ…è§£è¯»
        3. åœ¨æç«¯ç±»åˆ«ä¸å¹³è¡¡çš„æƒ…å†µä¸‹ï¼Œå‡†ç¡®ç‡å¯èƒ½ä¼šäº§ç”Ÿè¯¯å¯¼ï¼Œåº”æ›´å…³æ³¨ç²¾ç¡®ç‡ã€å¬å›ç‡å’ŒF1åˆ†æ•°
        4. æ··æ·†çŸ©é˜µå¯¹è§’çº¿ä¸Šçš„å€¼(TP, TN)è¶Šé«˜ï¼Œéå¯¹è§’çº¿å€¼(FP, FN)è¶Šä½ï¼Œæ¨¡å‹æ€§èƒ½è¶Šå¥½
        5. åœ¨è‚¡ç¥¨é¢„æµ‹åœºæ™¯ä¸­ï¼Œä¸åŒæŒ‡æ ‡çš„é‡è¦æ€§åº”åŸºäºäº¤æ˜“ç­–ç•¥æƒè¡¡ï¼Œä¾‹å¦‚:
           - è¿½æ±‚ç¨³å¥ç­–ç•¥: å…³æ³¨ç²¾ç¡®ç‡ï¼Œé™ä½é”™è¯¯ä¹°å…¥ä¿¡å·
           - è¿½æ±‚é«˜è¦†ç›–ç­–ç•¥: å…³æ³¨å¬å›ç‡ï¼Œç¡®ä¿ä¸é”™è¿‡ä¸Šæ¶¨æœºä¼š
    """
    try:
        if predictions is None or y_true is None:
            logger.error("Empty predictions or true labels")
            return {}
            
        if len(predictions) != len(y_true):
            logger.error(f"Length mismatch: predictions={len(predictions)}, y_true={len(y_true)}")
            return {}
            
        if len(predictions) == 0:
            logger.error("Empty data")
            return {}
            
        # Calculate accuracy
        accuracy = sum(predictions == y_true) / len(y_true) * 100
        
        # Calculate precision, recall, and F1 score
        tp = sum((predictions == 1) & (y_true == 1))
        fp = sum((predictions == 1) & (y_true == 0))
        fn = sum((predictions == 0) & (y_true == 1))
        tn = sum((predictions == 0) & (y_true == 0))
        
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0
        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
        
        # Calculate correct up and down predictions
        correct_up = tp
        correct_down = tn
        
        # Calculate confusion matrix
        cm = [[tn, fp], [fn, tp]]
        
        metrics = {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'correct_up': correct_up,
            'correct_down': correct_down,
            'confusion_matrix': cm
        }
        
        logger.info(f"Calculated performance metrics: Accuracy={accuracy:.2f}%, F1 score={f1:.4f}")
        
        return metrics
        
    except Exception as e:
        logger.error(f"Error calculating performance metrics: {str(e)}")
        return {}


def save_results(results, filepath, append=False):
    """
    Save prediction results to file
    
    Parameters:
        results (dict): Results dictionary
        filepath (str): File path to save
        append (bool): Whether to append to existing file
    
    Returns:
        bool: Whether the operation was successful
    """
    try:
        # Create directory if it does not exist
        directory = os.path.dirname(filepath)
        if directory and not os.path.exists(directory):
            os.makedirs(directory)
            
        # Format results for storage
        formatted_results = {k: v for k, v in results.items() if isinstance(v, (str, int, float, bool, list, dict))}
        
        # Convert numpy types
        def convert_numpy(obj):
            if isinstance(obj, np.ndarray):
                return obj.tolist()
            elif isinstance(obj, np.integer):
                return int(obj)
            elif isinstance(obj, np.floating):
                return float(obj)
            elif isinstance(obj, dict):
                return {k: convert_numpy(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [convert_numpy(i) for i in obj]
            else:
                return obj
                
        formatted_results = convert_numpy(formatted_results)
        
        # Add timestamp
        formatted_results['timestamp'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        
        mode = 'a' if append else 'w'
        with open(filepath, mode, encoding='utf-8') as f:
            if append:
                f.write('\n')
            json.dump(formatted_results, f, ensure_ascii=False, indent=2)
            
        logger.info(f"Results saved to: {filepath}")
        
        return True
        
    except Exception as e:
        logger.error(f"Error saving results: {str(e)}")
        return False


def create_stop_loss_profit_strategy(predictions, close_prices, stop_loss_pct=0.05, take_profit_pct=0.1):
    """
    åˆ›å»ºæ­¢æŸæ­¢ç›ˆäº¤æ˜“ç­–ç•¥ï¼ŒåŸºäºé¢„æµ‹ç»“æœå’Œä»·æ ¼æ•°æ®ç”Ÿæˆäº¤æ˜“ä¿¡å·
    
    è¯¥å‡½æ•°æ ¹æ®æ¨¡å‹é¢„æµ‹ç»“æœï¼Œç»“åˆæ­¢æŸå’Œæ­¢ç›ˆæ¡ä»¶ï¼Œç”Ÿæˆå®Œæ•´çš„äº¤æ˜“ä¿¡å·åºåˆ—ã€‚
    ç­–ç•¥ä¼šåœ¨é¢„æµ‹ä¸Šæ¶¨æ—¶äº§ç”Ÿä¹°å…¥ä¿¡å·ï¼Œå¹¶åœ¨è§¦å‘æ­¢æŸã€æ­¢ç›ˆæˆ–åè½¬ä¿¡å·æ—¶äº§ç”Ÿå–å‡ºä¿¡å·ï¼Œ
    é€‚ç”¨äºè‡ªåŠ¨åŒ–äº¤æ˜“ç³»ç»Ÿæˆ–å›æµ‹åˆ†æã€‚
    
    å‚æ•°:
        predictions (numpy.ndarray): æ¨¡å‹é¢„æµ‹ç»“æœæ•°ç»„ï¼Œå½¢çŠ¶ä¸º(n_samples,)ã€‚
                                   åº”ä¸ºäºŒå…ƒåˆ†ç±»ç»“æœï¼Œå…¶ä¸­0è¡¨ç¤ºé¢„æµ‹ä¸‹è·Œï¼Œ1è¡¨ç¤º
                                   é¢„æµ‹ä¸Šæ¶¨ã€‚æ•°ç»„é¡ºåºåº”ä¸close_pricesä¸€è‡´ï¼Œ
                                   ç¡®ä¿æ—¶é—´å¯¹é½ã€‚
        
        close_prices (numpy.ndarray): æ”¶ç›˜ä»·æ•°ç»„ï¼Œå½¢çŠ¶ä¸º(n_samples,)ã€‚
                                    å¿…é¡»ä¸predictionså…·æœ‰ç›¸åŒé•¿åº¦ï¼Œä¸”é¡ºåºä¸€è‡´ï¼Œ
                                    è¡¨ç¤ºæ¯ä¸ªäº¤æ˜“æ—¥çš„æ”¶ç›˜ä»·ã€‚
        
        stop_loss_pct (float): æ­¢æŸç™¾åˆ†æ¯”ï¼Œé»˜è®¤ä¸º0.05(5%)ã€‚å½“æŒä»“åä»·æ ¼ä¸‹è·Œ
                              è¶…è¿‡æ­¤æ¯”ä¾‹æ—¶è§¦å‘æ­¢æŸå–å‡ºã€‚è¾ƒå°çš„å€¼æ„å‘³ç€æ›´ä¸¥æ ¼çš„
                              é£é™©æ§åˆ¶ï¼Œä½†å¯èƒ½å¯¼è‡´æ›´é¢‘ç¹çš„äº¤æ˜“ã€‚èŒƒå›´é€šå¸¸ä¸º0.01-0.10ã€‚
        
        take_profit_pct (float): æ­¢ç›ˆç™¾åˆ†æ¯”ï¼Œé»˜è®¤ä¸º0.1(10%)ã€‚å½“æŒä»“åä»·æ ¼ä¸Šæ¶¨
                                è¶…è¿‡æ­¤æ¯”ä¾‹æ—¶è§¦å‘æ­¢ç›ˆå–å‡ºã€‚è¾ƒå¤§çš„å€¼æ„å‘³ç€æ›´å®½æ¾çš„
                                åˆ©æ¶¦ç›®æ ‡ï¼Œå¯èƒ½è®©ç›ˆåˆ©æŒç»­æ›´é•¿æ—¶é—´ã€‚èŒƒå›´é€šå¸¸ä¸º0.05-0.30ã€‚
    
    è¿”å›:
        pandas.DataFrame: åŒ…å«ä»¥ä¸‹åˆ—çš„DataFrame:
            - close: æ”¶ç›˜ä»·
            - prediction: åŸå§‹é¢„æµ‹ç»“æœ(0æˆ–1)
            - signal: äº¤æ˜“ä¿¡å·ï¼Œå…¶ä¸­:
                      0=æŒä»“ä¸å˜(æ— æ“ä½œ)
                      1=ä¹°å…¥ä¿¡å·
                      -1=å–å‡ºä¿¡å·
            - exit_reason: å–å‡ºåŸå› (ä»…å½“signal=-1æ—¶å­˜åœ¨)ï¼Œå¯èƒ½çš„å€¼åŒ…æ‹¬:
                          'stop_loss'=æ­¢æŸè§¦å‘
                          'take_profit'=æ­¢ç›ˆè§¦å‘
                          'reverse_signal'=é¢„æµ‹ä¿¡å·åè½¬
    
    ç­–ç•¥é€»è¾‘:
        1. åˆå§‹çŠ¶æ€: æœªæŒä»“ï¼Œç­‰å¾…ä¹°å…¥ä¿¡å·
        2. ä¹°å…¥æ¡ä»¶: é¢„æµ‹ç»“æœä¸ºä¸Šæ¶¨(1)ä¸”å½“å‰æœªæŒä»“
        3. å–å‡ºæ¡ä»¶(ä»¥ä¸‹ä»»ä¸€æ¡ä»¶æ»¡è¶³æ—¶):
           a. æ­¢æŸ: ä»·æ ¼ä¸‹è·Œè¶…è¿‡ä¹°å…¥ä»·çš„stop_loss_pct
           b. æ­¢ç›ˆ: ä»·æ ¼ä¸Šæ¶¨è¶…è¿‡ä¹°å…¥ä»·çš„take_profit_pct
           c. åè½¬ä¿¡å·: é¢„æµ‹ç”±ä¸Šæ¶¨å˜ä¸ºä¸‹è·Œ
        4. ä¹°å…¥åä¼šè®°å½•å…¥åœºä»·æ ¼ï¼Œå¹¶è®¡ç®—æ­¢æŸå’Œæ­¢ç›ˆç›®æ ‡ä»·ä½
        5. æ¯ä¸ªäº¤æ˜“æ—¥æ£€æŸ¥æ˜¯å¦æ»¡è¶³ä¸Šè¿°æ¡ä»¶ï¼Œå¹¶ç›¸åº”æ›´æ–°signalå€¼
    
    ç¤ºä¾‹:
        >>> # å‡è®¾å·²æœ‰é¢„æµ‹ç»“æœå’Œæ”¶ç›˜ä»·æ•°æ®
        >>> predictions = np.array([0, 0, 1, 1, 1, 0, 0, 1, 1, 0])
        >>> close_prices = np.array([100, 99, 101, 105, 103, 101, 98, 100, 106, 104])
        >>> 
        >>> # åˆ›å»ºæ›´ä¸¥æ ¼çš„æ­¢æŸç­–ç•¥(æ­¢æŸ3%ï¼Œæ­¢ç›ˆ15%)
        >>> df = create_stop_loss_profit_strategy(
        ...     predictions=predictions,
        ...     close_prices=close_prices,
        ...     stop_loss_pct=0.03,
        ...     take_profit_pct=0.15
        ... )
        >>> 
        >>> # æŸ¥çœ‹ç”Ÿæˆçš„äº¤æ˜“ä¿¡å·
        >>> print(df[['close', 'prediction', 'signal', 'exit_reason']])
        >>> 
        >>> # ç»Ÿè®¡å„ç±»ä¿¡å·
        >>> print(f"ä¹°å…¥ä¿¡å·æ•°é‡: {sum(df['signal'] == 1)}")
        >>> print(f"æ­¢æŸå–å‡ºæ¬¡æ•°: {sum((df['signal'] == -1) & (df['exit_reason'] == 'stop_loss'))}")
        >>> print(f"æ­¢ç›ˆå–å‡ºæ¬¡æ•°: {sum((df['signal'] == -1) & (df['exit_reason'] == 'take_profit'))}")
        >>> print(f"ä¿¡å·åè½¬å–å‡ºæ¬¡æ•°: {sum((df['signal'] == -1) & (df['exit_reason'] == 'reverse_signal'))}")
    
    æ³¨æ„:
        1. è¯¥ç­–ç•¥åŸºäºå†å²æ•°æ®å’Œé¢„æµ‹ç»“æœï¼Œå®é™…äº¤æ˜“ä¸­å¯èƒ½é¢ä¸´æ»‘ç‚¹ã€æµåŠ¨æ€§ç­‰é¢å¤–å› ç´ å½±å“
        2. æ­¢æŸæ­¢ç›ˆå‚æ•°åº”æ ¹æ®æ ‡çš„æ³¢åŠ¨æ€§ã€æŒä»“æ—¶é—´å’Œé£é™©åå¥½è°ƒæ•´ï¼Œä¸å­˜åœ¨é€šç”¨çš„æœ€ä½³å‚æ•°
        3. å‡½æ•°å‡è®¾ä»·æ ¼å’Œé¢„æµ‹æŒ‰æ—¶é—´é¡ºåºæ’åˆ—ï¼Œå¹¶ä¸”æ˜¯æ¯æ—¥é¢‘ç‡æ•°æ®ï¼›ä¸åŒé¢‘ç‡æ•°æ®å¯èƒ½éœ€è¦è°ƒæ•´é€»è¾‘
        4. è¿”å›çš„DataFrameä¸ä¼šç§»é™¤é¢„æµ‹å€¼ä¸ºNaNçš„è¡Œï¼Œå¦‚æœ‰å¿…è¦éœ€åœ¨è°ƒç”¨å‰é¢„å¤„ç†æ•°æ®
        5. è¯¥ç­–ç•¥ä¸è€ƒè™‘äº¤æ˜“é‡å’Œèµ„é‡‘ç®¡ç†ï¼Œå®é™…åº”ç”¨æ—¶åº”ç»“åˆèµ„é‡‘ç®¡ç†ç­–ç•¥ä½¿ç”¨
        6. å¯¹äºå¤šä¸ªæ ‡çš„çš„ç»„åˆç­–ç•¥ï¼Œå»ºè®®åˆ†åˆ«è®¡ç®—ä¿¡å·å†ç»¼åˆè€ƒè™‘ä»“ä½åˆ†é…
    """
    try:
        if predictions is None or len(predictions) == 0:
            logger.error("Empty predictions")
            return None
            
        if close_prices is None or len(close_prices) == 0:
            logger.error("Empty close prices")
            return None
            
        if len(predictions) != len(close_prices):
            logger.error(f"Length mismatch: predictions={len(predictions)}, close_prices={len(close_prices)}")
            return None
            
        # Create result DataFrame
        df = pd.DataFrame({
            'close': close_prices,
            'prediction': predictions,
            'signal': 0  # Initialize signal
        })
        
        # Initialize variables
        in_position = False
        entry_price = 0
        stop_loss = 0
        take_profit = 0
        
        # Daily analysis
        for i in range(len(df)):
            # If not in position and prediction is up
            if not in_position and df['prediction'].iloc[i] == 1:
                # Buy signal
                df.loc[df.index[i], 'signal'] = 1
                in_position = True
                entry_price = df['close'].iloc[i]
                stop_loss = entry_price * (1 - stop_loss_pct)
                take_profit = entry_price * (1 + take_profit_pct)
                
            # If in position
            elif in_position:
                current_price = df['close'].iloc[i]
                
                # Check stop-loss condition
                if current_price <= stop_loss:
                    df.loc[df.index[i], 'signal'] = -1  # Trigger stop-loss
                    df.loc[df.index[i], 'exit_reason'] = 'stop_loss'
                    in_position = False
                    
                # Check take-profit condition
                elif current_price >= take_profit:
                    df.loc[df.index[i], 'signal'] = -1  # Trigger take-profit
                    df.loc[df.index[i], 'exit_reason'] = 'take_profit'
                    in_position = False
                    
                # Check reverse signal
                elif df['prediction'].iloc[i] == 0:
                    df.loc[df.index[i], 'signal'] = -1  # Prediction is down, sell
                    df.loc[df.index[i], 'exit_reason'] = 'reverse_signal'
                    in_position = False
                    
        # Calculate stop-loss and take-profit trigger counts
        stop_loss_count = sum((df['signal'] == -1) & (df['exit_reason'] == 'stop_loss'))
        take_profit_count = sum((df['signal'] == -1) & (df['exit_reason'] == 'take_profit'))
        reverse_count = sum((df['signal'] == -1) & (df['exit_reason'] == 'reverse_signal'))
        
        logger.info(f"Stop-loss and take-profit strategy: Stop-loss count={stop_loss_count}, Take-profit count={take_profit_count}, Reverse signal count={reverse_count}")
        
        return df
        
    except Exception as e:
        logger.error(f"Error creating stop-loss and take-profit strategy: {str(e)}")
        return None


def apply_stop_loss_take_profit(signals, close_prices, stop_loss=0.05, take_profit=0.1):
    """
    Apply stop-loss and take-profit strategy to existing trading signals
    
    Parameters:
        signals (numpy.ndarray): Array of trading signals where 1=buy, -1=sell, 0=hold
        close_prices (numpy.ndarray): Array of close prices corresponding to signals
        stop_loss (float): Stop loss percentage (default: 0.05 or 5%)
        take_profit (float): Take profit percentage (default: 0.1 or 10%)
        
    Returns:
        numpy.ndarray: Modified signals array with stop-loss and take-profit applied
    """
    try:
        if len(signals) != len(close_prices):
            raise ValueError(f"Length mismatch: signals={len(signals)}, close_prices={len(close_prices)}")
        
        # Make a copy of the signals to avoid modifying the original array
        modified_signals = signals.copy()
        
        # Keep track of the position status and entry price
        in_position = False
        entry_price = 0
        
        for i in range(len(signals)):
            # Current price
            current_price = close_prices[i]
            
            # If not in position and we have a buy signal
            if not in_position and signals[i] == 1:
                in_position = True
                entry_price = current_price
                # Keep the original buy signal
                
            # If in position, check for stop loss or take profit
            elif in_position:
                # Calculate price change since entry
                price_change = (current_price - entry_price) / entry_price
                
                # Stop loss triggered
                if price_change <= -stop_loss:
                    modified_signals[i] = -1  # Sell signal
                    in_position = False
                
                # Take profit triggered
                elif price_change >= take_profit:
                    modified_signals[i] = -1  # Sell signal
                    in_position = False
                
                # Original sell signal
                elif signals[i] == -1:
                    # Keep the original sell signal
                    in_position = False
        
        return modified_signals
        
    except Exception as e:
        logging.error(f"Error applying stop-loss and take-profit: {str(e)}")
        return signals  # Return original signals on error


def format_time(seconds):
    """
    Format time in human-readable format
    
    Parameters:
        seconds (float): Time in seconds
    
    Returns:
        str: Formatted time string
    """
    if seconds < 60:
        return f"{seconds:.1f} seconds"
    elif seconds < 3600:
        minutes = seconds / 60
        return f"{minutes:.1f} minutes"
    else:
        hours = seconds / 3600
        return f"{hours:.1f} hours"


def generate_advanced_analysis(predictions, metrics, model_info, stock_code, args, trading_signals=None, returns_eval=None):
    """
    ç”Ÿæˆé«˜çº§åˆ†æè§£è¯»ï¼Œæä¾›è¯¦ç»†çš„æ¨¡å‹é¢„æµ‹è§£é‡Šå’Œäº¤æ˜“å»ºè®®
    
    å‚æ•°:
        predictions (list/array): é¢„æµ‹ç»“æœ
        metrics (dict): æ¨¡å‹è¯„ä¼°æŒ‡æ ‡
        model_info (dict/object): æ¨¡å‹ç›¸å…³ä¿¡æ¯
        stock_code (str): è‚¡ç¥¨ä»£ç 
        args (object): ç¨‹åºå‚æ•°
        trading_signals (list/array, optional): äº¤æ˜“ä¿¡å·
        returns_eval (dict, optional): æ”¶ç›Šè¯„ä¼°ç»“æœ
        
    è¿”å›:
        str: è¯¦ç»†çš„åˆ†ææŠ¥å‘Šæ–‡æœ¬
    """
    # ä¿è¯é¢„æµ‹ç»“æœæ˜¯åˆ—è¡¨ç±»å‹
    if predictions is not None and hasattr(predictions, 'tolist'):
        recent_preds = predictions[-args.predict_days:].tolist()
    elif predictions is not None:
        recent_preds = predictions[-args.predict_days:]
    else:
        recent_preds = []
    
    # åˆ†æé¢„æµ‹è¶‹åŠ¿
    if len(recent_preds) > 0:
        up_days = sum(1 for p in recent_preds if p > 0)
        down_days = sum(1 for p in recent_preds if p < 0)
        flat_days = len(recent_preds) - up_days - down_days
        
        # è®¡ç®—ä¿¡å·å¼ºåº¦
        signal_strength = sum(abs(p) for p in recent_preds) / len(recent_preds)
        
        # ç¡®å®šæ€»ä½“è¶‹åŠ¿
        if up_days > down_days:
            trend = "ä¸Šæ¶¨"
        elif down_days > up_days:
            trend = "ä¸‹è·Œ"
        else:
            trend = "éœ‡è¡"
            
        # å¼ºåº¦è¯„çº§
        if signal_strength > 0.015:
            strength = "å¼º"
        elif signal_strength > 0.005:
            strength = "ä¸­"
        else:
            strength = "å¼±"
    else:
        return "é¢„æµ‹æ•°æ®ä¸è¶³ï¼Œæ— æ³•ç”Ÿæˆè¯¦ç»†åˆ†æã€‚"
    
    # æ„å»ºé«˜çº§åˆ†ææŠ¥å‘Š
    analysis = []
    analysis.append(f"## ğŸ” {stock_code}è¯¦ç»†åˆ†æè§£è¯»\n")
    
    # 1. æ¨¡å‹é¢„æµ‹å¼ºåº¦åˆ†æ
    analysis.append("### 1ï¸âƒ£ æ¨¡å‹é¢„æµ‹å¼ºåº¦åˆ†æ\n")
    accuracy = metrics.get('accuracy', 0) * 100
    analysis.append(f"- **é¢„æµ‹å‡†ç¡®ç‡**: æ¨¡å‹æ•´ä½“å‡†ç¡®ç‡è¾¾åˆ°{accuracy:.2f}%ï¼Œ")
    
    if accuracy > 70:
        analysis.append("è¿™åœ¨è‚¡ç¥¨é¢„æµ‹ä¸­å±äºéå¸¸é«˜çš„æ°´å¹³ï¼Œå¤§å¤§æé«˜äº†ç­–ç•¥å¯ä¿¡åº¦")
    elif accuracy > 60:
        analysis.append("è¿™åœ¨è‚¡ç¥¨é¢„æµ‹ä¸­å±äºè¾ƒå¥½çš„æ°´å¹³ï¼ˆè€ƒè™‘åˆ°å¸‚åœºéšæœºæ€§ï¼‰")
    elif accuracy > 50:
        analysis.append("è¿™åœ¨è‚¡ç¥¨é¢„æµ‹ä¸­å±äºå¯æ¥å—çš„æ°´å¹³")
    else:
        analysis.append("è¿™åœ¨è‚¡ç¥¨é¢„æµ‹ä¸­å‡†ç¡®ç‡åä½ï¼Œå»ºè®®è°¨æ…å‚è€ƒ")
    
    # åˆ†æè¿ç»­æ€§ä¿¡å·
    if up_days == len(recent_preds):
        analysis.append(f"- **è¿ç»­ä¸Šæ¶¨ä¿¡å·**: æœªæ¥{args.predict_days}å¤©å…¨éƒ¨é¢„æµ‹ä¸ºä¸Šæ¶¨èµ°åŠ¿ï¼Œè¿™ç§ä¸€è‡´æ€§ä¿¡å·éå¸¸ç½•è§")
    elif down_days == len(recent_preds):
        analysis.append(f"- **è¿ç»­ä¸‹è·Œä¿¡å·**: æœªæ¥{args.predict_days}å¤©å…¨éƒ¨é¢„æµ‹ä¸ºä¸‹è·Œèµ°åŠ¿ï¼Œè¿™ç§ä¸€è‡´æ€§ä¿¡å·éå¸¸ç½•è§")
    else:
        major_trend = "ä¸Šæ¶¨" if up_days > down_days else "ä¸‹è·Œ"
        major_days = max(up_days, down_days)
        analysis.append(f"- **è¶‹åŠ¿ä¿¡å·**: æœªæ¥{args.predict_days}å¤©ä¸­æœ‰{major_days}å¤©é¢„æµ‹ä¸º{major_trend}èµ°åŠ¿")
    
    # ä¿¡å·å¼ºåº¦è¯„çº§
    analysis.append(f"- **ä¿¡å·å¼ºåº¦è¯„çº§**: ä¿¡å·å¼ºåº¦ä¸º\"{strength}\"çº§åˆ«ï¼Œ")
    if strength == "å¼º":
        analysis.append("è¿™æ„å‘³ç€é¢„æµ‹çš„å¹³å‡æ¶¨è·Œå¹…ç›¸å¯¹è¾ƒé«˜")
    elif strength == "ä¸­":
        analysis.append("è¿™è¡¨ç¤ºé¢„æµ‹çš„å¹³å‡æ¶¨è·Œå¹…å¤„äºä¸­ç­‰æ°´å¹³")
    else:
        analysis.append("è¿™è¡¨ç¤ºé¢„æµ‹çš„å¹³å‡æ¶¨è·Œå¹…è¾ƒå°")
    
    # é›†æˆæ¨¡å‹ä¿¡æ¯
    if hasattr(args, 'ensemble') and args.ensemble:
        analysis.append("- **é›†æˆæ¨¡å‹å†³ç­–**: æœ¬é¢„æµ‹ç»“æœæ¥è‡ªå¤šæ¨¡å‹é›†æˆæ–¹æ³•ï¼Œ")
        if hasattr(model_info, 'estimator_names_') and len(model_info.estimator_names_) > 0:
            model_names = ', '.join(model_info.estimator_names_)
            analysis.append(f"- åŒ…å«äº†{model_names}ç­‰ç®—æ³•çš„'æŠ•ç¥¨'ç»“æœï¼Œè¿™ç§é›†æˆæ–¹æ³•é€šå¸¸æ¯”å•ä¸€æ¨¡å‹æ›´å¯é ")
        else:
            analysis.append("æ±‡é›†äº†å¤šç§ç®—æ³•çš„é¢„æµ‹ç»“æœï¼Œé€šå¸¸æ¯”å•ä¸€æ¨¡å‹æ›´å¯é ")
            
    # 2. æŠ€æœ¯æŒ‡æ ‡æ¨æ–­
    analysis.append("\n### 2ï¸âƒ£ æŠ€æœ¯æŒ‡æ ‡ä½è¯\n")
    analysis.append("åŸºäºæ¨¡å‹è®­ç»ƒè¿‡ç¨‹å’Œé¢„æµ‹ç»“æœï¼Œæ¨æ–­å¯èƒ½çš„æŠ€æœ¯æŒ‡æ ‡çŠ¶æ€ï¼š\n")
    
    if trend == "ä¸Šæ¶¨" and strength == "å¼º":
        analysis.append("""
- **ç›¸å¯¹å¼ºå¼±æŒ‡æ ‡(RSI)**: å¯èƒ½å¤„äºä¸Šå‡è¶‹åŠ¿ä½†æœªè¾¾åˆ°è¿‡åº¦ä¹°å…¥åŒºåŸŸ
- **MACDæŒ‡æ ‡**: å¯èƒ½å‡ºç°é‡‘å‰æˆ–æ­£åœ¨å½¢æˆå¤šå¤´æ ¼å±€
- **æˆäº¤é‡å˜åŒ–**: å¯èƒ½ä¼´éšç€ä¸Šæ¶¨ä¿¡å·çš„æ”¾é‡è¿¹è±¡
- **å¸ƒæ—å¸¦ä½ç½®**: ä»·æ ¼å¯èƒ½çªç ´ä¸­è½¨å‘ä¸Šè½¨è¿è¡Œ
""")
    elif trend == "ä¸‹è·Œ" and strength == "å¼º":
        analysis.append("""
- **ç›¸å¯¹å¼ºå¼±æŒ‡æ ‡(RSI)**: å¯èƒ½å¤„äºä¸‹é™è¶‹åŠ¿ä½†æœªè¾¾åˆ°è¿‡åº¦å–å‡ºåŒºåŸŸ
- **MACDæŒ‡æ ‡**: å¯èƒ½å‡ºç°æ­»å‰æˆ–æ­£åœ¨å½¢æˆç©ºå¤´æ ¼å±€
- **æˆäº¤é‡å˜åŒ–**: å¯èƒ½ä¼´éšç€ä¸‹è·Œä¿¡å·çš„æ”¾é‡è¿¹è±¡
- **å¸ƒæ—å¸¦ä½ç½®**: ä»·æ ¼å¯èƒ½çªç ´ä¸­è½¨å‘ä¸‹è½¨è¿è¡Œ
""")
    elif trend == "éœ‡è¡":
        analysis.append("""
- **ç›¸å¯¹å¼ºå¼±æŒ‡æ ‡(RSI)**: å¯èƒ½åœ¨ä¸­æ€§åŒºåŸŸæ³¢åŠ¨
- **MACDæŒ‡æ ‡**: å¯èƒ½åœ¨é›¶è½´é™„è¿‘æ³¢åŠ¨ï¼Œæ— æ˜æ˜¾è¶‹åŠ¿
- **æˆäº¤é‡å˜åŒ–**: å¯èƒ½æˆäº¤é‡è¾ƒä¸ºå¹³ç¨³
- **å¸ƒæ—å¸¦ä½ç½®**: ä»·æ ¼å¯èƒ½åœ¨ä¸­è½¨é™„è¿‘æ³¢åŠ¨
""")
    else:
        # å…¶ä»–ç»„åˆæƒ…å†µ
        analysis.append("""
- **ç›¸å¯¹å¼ºå¼±æŒ‡æ ‡(RSI)**: éœ€è¿›ä¸€æ­¥ç›‘æµ‹ï¼Œç›®å‰æ— æ˜ç¡®æŒ‡å‘
- **MACDæŒ‡æ ‡**: éœ€è¿›ä¸€æ­¥ç¡®è®¤è¶‹åŠ¿å½¢æˆ
- **æˆäº¤é‡å˜åŒ–**: å»ºè®®å…³æ³¨åç»­æˆäº¤é‡å˜åŒ–ç¡®è®¤ä¿¡å·
""")
    
    # 3. å®é™…åº”ç”¨å»ºè®®
    analysis.append("\n### 3ï¸âƒ£ å®é™…åº”ç”¨å»ºè®®\n")
    
    if trend == "ä¸Šæ¶¨":
        if strength == "å¼º":
            analysis.append("""
- **åˆ†æ‰¹ä¹°å…¥ç­–ç•¥**: è€ƒè™‘å°†èµ„é‡‘åˆ†ä¸º3-4ä»½ï¼Œåœ¨æœªæ¥2-3ä¸ªäº¤æ˜“æ—¥å†…åˆ†æ‰¹ä¹°å…¥
- **æ­¢æŸä½è®¾ç½®**: ä¸¥æ ¼è®¾ç½®åœ¨å½“å‰ä»·æ ¼çš„95%ä½ç½®
- **ç›®æ ‡ä»·ä½**: å‚è€ƒæ¨¡å‹é¢„æµ‹çš„ä¸Šæ¶¨è¶‹åŠ¿ï¼Œå¯ä»¥è®¾å®š5-10%çš„ç›ˆåˆ©ç›®æ ‡
- **æŒä»“æ—¶é—´**: å»ºè®®æŒæœ‰5-7ä¸ªäº¤æ˜“æ—¥ï¼Œä¸é¢„æµ‹å‘¨æœŸç›¸å¯¹åº”
- **é£é™©æ•å£æ§åˆ¶**: å•æ¬¡äº¤æ˜“èµ„é‡‘ä¸è¶…è¿‡æ€»èµ„é‡‘çš„15-20%
""")
        elif strength == "ä¸­":
            analysis.append("""
- **è½»ä»“è¯•æ¢ç­–ç•¥**: è€ƒè™‘ç”¨è¾ƒå°ä»“ä½è¿›è¡Œè¯•æ¢æ€§ä¹°å…¥
- **æ­¢æŸä½è®¾ç½®**: å»ºè®®è®¾ç½®åœ¨å½“å‰ä»·æ ¼çš„97%ä½ç½®
- **ç›®æ ‡ä»·ä½**: è®¾å®š3-7%çš„ç›ˆåˆ©ç›®æ ‡
- **æŒä»“æ—¶é—´**: å»ºè®®æŒæœ‰3-5ä¸ªäº¤æ˜“æ—¥ï¼Œéšæ—¶å‡†å¤‡æ­¢ç›ˆ
- **é£é™©æ•å£æ§åˆ¶**: å•æ¬¡äº¤æ˜“èµ„é‡‘ä¸è¶…è¿‡æ€»èµ„é‡‘çš„10%
""")
        else:
            analysis.append("""
- **è§‚æœ›ä¸ºä¸»ç­–ç•¥**: è¶‹åŠ¿è™½ä¸ºä¸Šæ¶¨ä½†ä¿¡å·è¾ƒå¼±ï¼Œå»ºè®®ä»¥è§‚æœ›ä¸ºä¸»
- **æ¡ä»¶å•è®¾ç½®**: å¯è®¾ç½®çªç ´æ¡ä»¶å•ï¼Œç¡®è®¤çªç ´åå†ä»‹å…¥
- **æ­¢æŸä½è®¾ç½®**: è‹¥ä»‹å…¥ï¼Œå»ºè®®è®¾ç½®è¾ƒç´§çš„æ­¢æŸåœ¨å½“å‰ä»·æ ¼çš„98%ä½ç½®
- **é£é™©æ•å£æ§åˆ¶**: è‹¥ä»‹å…¥ï¼Œå»ºè®®ä½¿ç”¨ä¸è¶…è¿‡æ€»èµ„é‡‘5%çš„å°ä»“ä½
""")
    elif trend == "ä¸‹è·Œ":
        if strength == "å¼º":
            analysis.append("""
- **å›é¿ç­–ç•¥**: ä¸å»ºè®®ç°åœ¨ä¹°å…¥ï¼Œå¸‚åœºçœ‹è·Œä¿¡å·æ˜ç¡®
- **æŒå¸è§‚æœ›**: ç­‰å¾…æ›´å¥½çš„ä¹°å…¥æ—¶æœºæˆ–è€ƒè™‘å…¶ä»–æŠ•èµ„æ ‡çš„
- **å¯¹å†²ç­–ç•¥**: å¦‚æœ‰ç›¸å…³å·¥å…·ï¼Œå¯è€ƒè™‘é€‚å½“åšç©ºå¯¹å†²
- **å‡ä»“è®¡åˆ’**: è‹¥å·²æŒæœ‰ï¼Œå»ºè®®åˆ¶å®šé˜¶æ®µæ€§å‡ä»“è®¡åˆ’
""")
        elif strength == "ä¸­":
            analysis.append("""
- **è§‚æœ›ç­–ç•¥**: ä¸å»ºè®®ç°åœ¨ä¹°å…¥ï¼Œå¸‚åœºä¸‹è¡Œè¶‹åŠ¿æ˜æ˜¾
- **æ­¢æŸè®¾ç½®**: å¦‚å·²æŒæœ‰ï¼Œå»ºè®®è®¾ç½®æ­¢æŸä½ä¿æŠ¤ä»“ä½
- **å‡ä»“è§„åˆ’**: å¯è€ƒè™‘åœ¨åå¼¹æ—¶å‡è½»ä»“ä½
""")
        else:
            analysis.append("""
- **è°¨æ…ç­–ç•¥**: ä¿æŒè°¨æ…ï¼ŒçŸ­æœŸå†…å¸‚åœºå¯èƒ½æœ‰å°å¹…å›è°ƒ
- **åˆ†æ‰¹å‡ä»“**: å¦‚å·²æŒæœ‰ï¼Œå¯ä»¥åˆ¶å®šåˆ†æ‰¹å‡ä»“è®¡åˆ’
- **è§‚å¯Ÿæ”¯æ’‘**: å…³æ³¨è¿‘æœŸæ”¯æ’‘ä½è¡¨ç°ï¼Œå¯»æ‰¾ä¼ç¨³ä¿¡å·
""")
    else:  # éœ‡è¡
        analysis.append("""
- **åŒºé—´äº¤æ˜“ç­–ç•¥**: å¸‚åœºå¤„äºéœ‡è¡æœŸï¼Œå¯è€ƒè™‘åŒºé—´äº¤æ˜“ç­–ç•¥
- **æ”¯æ’‘é˜»åŠ›ä½é‡è§†**: ç‰¹åˆ«å…³æ³¨è¿‘æœŸå½¢æˆçš„æ”¯æ’‘ä½å’Œé˜»åŠ›ä½
- **è½»ä»“çŸ­çº¿**: å¯å°è¯•è½»ä»“çŸ­çº¿æ“ä½œï¼Œä¸¥æ§é£é™©
- **é¿å…é‡ä»“**: é¿å…é‡ä»“æ“ä½œï¼Œæ§åˆ¶å•ç¬”äº¤æ˜“é£é™©
""")
    
    # 4. é£é™©å› ç´ å‰–æ
    analysis.append("\n### 4ï¸âƒ£ é£é™©å› ç´ å‰–æ\n")
    
    analysis.append(f"- **æ¨¡å‹ç½®ä¿¡åº¦**: {accuracy:.2f}%çš„å‡†ç¡®ç‡æ„å‘³ç€ä»æœ‰çº¦{100-accuracy:.2f}%çš„é”™è¯¯æ¦‚ç‡")
    analysis.append("- **å¤–éƒ¨äº‹ä»¶é£é™©**: çªå‘æ”¿ç­–å˜åŒ–ã€å…¬å¸å…¬å‘Šæˆ–è¡Œä¸šé»‘å¤©é¹…äº‹ä»¶å¯èƒ½å¯¼è‡´é¢„æµ‹å¤±æ•ˆ")
    analysis.append("- **å¸‚åœºæµåŠ¨æ€§**: è‹¥å¸‚åœºæ•´ä½“æµåŠ¨æ€§æ”¶ç´§ï¼Œå¯èƒ½å½±å“é¢„æœŸæ¶¨è·Œå¹…å®ç°")
    
    # 5. å†³ç­–å»ºè®®æ€»ç»“
    analysis.append("\n### 5ï¸âƒ£ å†³ç­–å»ºè®®æ€»ç»“\n")
    
    if trend == "ä¸Šæ¶¨":
        analysis.append("âœ… **ä¹°å…¥ç†ç”±**:")
        if up_days == len(recent_preds):
            analysis.append(f"- æ¨¡å‹é¢„æµ‹æœªæ¥{args.predict_days}å¤©è¿ç»­ä¸Šæ¶¨")
        else:
            analysis.append(f"- æ¨¡å‹é¢„æµ‹æœªæ¥{args.predict_days}å¤©ä¸­{up_days}å¤©ä¸Šæ¶¨")
        analysis.append(f"- ä¿¡å·å¼ºåº¦ä¸º\"{strength}\"çº§åˆ«")
        analysis.append("- æ¨¡å‹éªŒè¯æ˜¾ç¤ºè¾ƒå¥½çš„é¢„æµ‹å‡†ç¡®æ€§")
    elif trend == "ä¸‹è·Œ":
        analysis.append("âŒ **å–å‡º/å›é¿ç†ç”±**:")
        if down_days == len(recent_preds):
            analysis.append(f"- æ¨¡å‹é¢„æµ‹æœªæ¥{args.predict_days}å¤©è¿ç»­ä¸‹è·Œ")
        else:
            analysis.append(f"- æ¨¡å‹é¢„æµ‹æœªæ¥{args.predict_days}å¤©ä¸­{down_days}å¤©ä¸‹è·Œ")
        analysis.append(f"- ä¿¡å·å¼ºåº¦ä¸º\"{strength}\"çº§åˆ«")
    else:
        analysis.append("âš–ï¸ **è§‚æœ›ç†ç”±**:")
        analysis.append("- æ¨¡å‹é¢„æµ‹å¸‚åœºå¤„äºéœ‡è¡çŠ¶æ€")
        analysis.append(f"- ä¸Šæ¶¨å¤©æ•°:{up_days}ï¼Œä¸‹è·Œå¤©æ•°:{down_days}ï¼ŒæŒå¹³:{flat_days}")
        analysis.append(f"- ä¿¡å·å¼ºåº¦ä¸º\"{strength}\"çº§åˆ«")
    
    analysis.append("\nâš ï¸ **é£é™©æ§åˆ¶**:")
    if trend == "ä¸Šæ¶¨":
        analysis.append(f"- è®¾ç½®ç´§å‡‘æ­¢æŸï¼ˆ{95+int(strength=='ä¸­')*2+int(strength=='å¼±')*3}%çš„å½“å‰ä»·æ ¼ï¼‰")
        analysis.append("- åˆ†æ‰¹å»ºä»“é™ä½æ—¶ç‚¹é£é™©")
    elif trend == "ä¸‹è·Œ":
        analysis.append("- é¿å…é€†åŠ¿æ“ä½œ")
        analysis.append("- å¦‚å·²æŒæœ‰ï¼ŒåŠæ—¶è®¾ç½®æ­¢æŸä½")
    analysis.append("- å¯†åˆ‡å…³æ³¨é‡èƒ½å˜åŒ–å’Œç›˜ä¸­å¼‚åŠ¨")
    
    # åˆå¹¶æ‰€æœ‰åˆ†ææ®µè½
    return "\n".join(analysis)
